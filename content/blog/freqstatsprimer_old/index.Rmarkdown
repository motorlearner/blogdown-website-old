---
# text
title:      "OLD Frequentist Statistics Primer"
subtitle:   "p-values, confidence intervals, and error rates"
excerpt:    "All things frequentist statistics."

# metadata (NB bottom will show "see also: all posts with same tag")
author:     "Aslan B."
date:       2022-04-20
categories: 
- statistics
tags:
- statistics

# other
layout:     single
draft:      false
---

```{r setup, include = FALSE}

# load knitr
library(knitr)
# set chunk opts
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  include = TRUE,
  echo    = TRUE
)
# set seed
set.seed(123)
# display numbers to two decimals
options(digits = 1)

```

```{css, echo=FALSE}
/* make all images centered;
   no width parameter to preserve width set via ggsave;
   fig caption text style is set manually each time,
    there is probably an easier way to do this like
    make a css text class with those style params and
    then just apply that class, but I am too lazy atm
*/
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

/*  FONT SIZE
    - p = main text
    - li = list (could also use ol and ul for ordered, unordered)
    - h = headings
*/
p {
  font-size: 14px;
}
li{
  font-size: 14px;
}
h1{
  font-size: 16px;
}
h2{
  font-size: 14px;
}
```

> üìù **Note**
Some note here

> ‚úîÔ∏è **Key Point**
Something key point here

> ‚ùå  **Beware**
Some common error here

> üß≠ **Orientation**
> Could use this as summary.

To do:

Come up with categories for common mistakes, which makes it easier to watch out for those.
For example: frequency misinterpretations (always recall that probability is frequency), 
missing the assumption (always check under which assumption something is computed).

Use coin flip examples as analogies throughout, because that's a natural example for frequencies.
Introduce this in the sampling distribution section. First, emphasize frequencies. Then, 
introduce coin flip as a natural easy example. You can flip a coin N times, count m the 
number of heads, probability of the coin is mu. Don't use excessively. Could even do 
boxes called "coin analogy". 

## In the long run

## Working Example

We will use the following toy example:

In `R`, there is a function `rnorm(n)` which draws a sample of size `n` from a normal
distribution with mean $\mu$ and standard deviation $\sigma$. I will call this the 
*population distribution*.

```{r}
rnorm(10)
```

We have some hypothesis or question about the population mean $\mu$. Besides knowing
that our population distribution is a normal distribution, we will also pretend to know
that the population standard deviation is $\sigma=1$. We will draw a sample 
of size $N$ from the population distribution, and use the sample mean $m$ to make some 
inference about $\mu$

<!-- GENERAL FRAMEWORK -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/01_overview.svg" />
  <b> Figure 1. </b> 
  Overview of the working example. We draw a sample of size \(N\) from a population 
  distribution with mean \(\mu\) and standard deviation \(\sigma\). We want to
  answer some question about \(\mu\). In our case, we know that the population 
  distribution is normal, and that \(\sigma=1\). 
</p>

Have a look at **Figure 1** and make sure you know what $\mu, \sigma, m, s, N$ stand for. 
Greek letters stand for *population* parameters, Latin letters stand for *sample* parameters. 
If a letter starts with an **m**, it stands for the **m**ean. 
If a letter starts with an **s**, it stands for the **s**tandard deviation.

|                    | Population | Sample |
| :----------------- | :--------: | :----: |
| Mean               | $\mu$      | $m$    |
| Standard Deviation | $\sigma$   | $s$    |
| Size               | $-$        | $N$    |

There are many questions we could ask about $\mu$. Is it greater/smaller than 
some specific value?  Is it different from / similar to some specific value? 
But first, we need to introduce the sampling distribution.

## The Sampling Distribution

Let's take a sample of size $N$ from a population distribution with mean $\mu$ and 
standard deviation $\sigma$. We can calculate the sample mean $m$. In **Figure 2**, you 
see this process repeated twenty times for three different sample sizes. Each line shows
a different sample. The gray points are the sample values and the colored square is their
mean.

<!-- CLT SAMPLING -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/02_clt_sampling.svg" />
  <b> Figure 2. </b> 
  Samples of size \(N\), drawn from a normal distribution with mean \(\mu\) and standard 
  deviation \(\sigma\). The squares are the sample means \(m\). The dashed line 
  highlights \(\mu\). The x-axis shows the distance from \(\mu\), measured in multiples 
  of \(\sigma\).
</p>

Note that the sample means break away farther from $\mu$ (dashed line) when $N$ is
smaller. Now, what if we repeat this process a gazillion times? We get a gazillion sample
means $m$, which form their own distribution. This distribution is called the 
*sampling distribution of $m$*. You can see it in **Figure 3**, together with the 
population distribution (which, just in our working example, is a normal distribution).

<!-- CLT DISTRIBUTION -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/03_clt_distributions.svg" />
  <b> Figure 3. </b> 
  The sampling distribution of \(m\), for three sample sizes \(N\). The samples 
  were drawn from the gray population distribution with mean \(\mu\) and standard deviation 
  \(\sigma\). The dashed line highlights \(\mu\). The x-axis shows the distance from 
  \(\mu\), measured in multiples of \(\sigma\).
</p>

There are a few things about the sampling distribution of $m$ that should immediately
catch your eye. 
First, it has the same mean as the population distribution. That is, it has mean $\mu$. 
Second, its spread decreases as the sample size increases. In fact, its standard deviation 
is $\frac{\sigma}{\sqrt{N}}$. Note that this specific standard deviation is also called 
the *standard error of $m$*.
Third---maybe not quite so obvious---it is a normal distribution. To sum up, the
sampling distribution of $m$ is a normal distribution with mean $\mu$ and standard 
deviation $\frac{\sigma}{\sqrt{N}}$. 

> üìù **Note:** The sampling distribution of $m$ is a normal distribution with mean $\mu$ and standard
> deviation $\frac{\sigma}{\sqrt{N}}$ *for almost any population distribution*, as long as
> $N$ is sufficiently large. The population distribution must have a finite mean $\mu$ and 
> standard deviation $\sigma$, but it does not need to be a normal distribution as in our working example!
> We will come back to this later.

The sampling distribution of $m$ is important because it gives us a sense of what sample 
mean $m$ to expect if we draw a random sample from the population distribution. We know 
the sampling distribution of $m$ as long as we know $\mu$, $\sigma$ and $N$. Of course, 
we don't know the value of $\mu$. We don't have to.

## Rejecting the Incompatible

Imagine we are at a scientific conference, and we want to know whether our common friend
Rick is also at the conference. We know that Rick is always wearing a lab coat. We look 
carefully, but no one is wearing a lab coat. 

1. If Rick is at the conference, then someone will be wearing a lab coat. 
2. No one is wearing a lab coat. 
3. Therefore, Rick is not at the conference.

This is a valid argument called *modus tollens*, also known as 'denying the consequent'. 
Now, let's say we *do* see someone wearing a lab coat.

1. If Rick is at the conference, then someone will be wearing a lab coat. 
2. Someone is wearing a lab coat. 
3. --

We cannot conclude that Rick was at the conference.
We might have seen Rick in his lab coat, or we might have 
seen someone else wearing a lab coat. We simply cannot conclude anything. 

Below you see these two cases summarized in more abstract form. We start with a 
model/assumption that tells us what observation (O) to expect under a certain possible 
truth (T). Then we make the observation. Finally, if this observation is incompatible
with the possible truth, we reject the possible truth.

|                     | Conclusive         | Inconclusive     |
| ------------------- | ------------------ | ---------------- |
| 1. Model/Assumption | If T, then O.      | If T, then O.    |
| 2. Observation      | Not O.             | O.               |
| 3. Conclusion       | Therefore, not T.  | ---              |

> üìù **Note:** 'If T, then O. Not O. Therefore, not T.' is a rock-solid argument only if
> the model/assumption ('If T, then O.') is true. In reality, we should also consider 
> the possibility that the model/assumption is wrong. 'If T, then O. Not O. Therefore,
> not T *and/or the statement "if T, then O" is wrong*.

This is the same logic that we'll use in our working example. However, this was a 
minimalist presentation that left some steps implicit. Here is the same logic with all 
steps made explicit:

1. *List all the possible truths.* <br>
$T_1, T_2, T_3, ...$
2. *List all the possible observations.* <br>
$O_1, O_2, O_3, ...$
3. *For each possible truth, define which observations are compatible/incompatible.* <br>
![](compatiblemapping.png)
4. *Make an observation.*
5. *Reject all the possible truths that are incompatible with the observation.*
6. *The actual truth is somewhere among the remaining possible truths (which are compatible with the observation).*

So you start with all possible truths, then make an observation, 
then _reject_ the possible truths that are _incompatible_ with the observation. You are
left with all the possible truths that are not rejected. The actual truth is somewhere in 
there. They key point is: the only way to conclude that the actual truth is *something* 
is to reject *everything else*. You can only reject!
If you have a lake full of fish, and you are looking for the one true 
fish, you cannot just cast your fishing pole and pull the true fish out of the lake. 
Instead, you have to remove all the other fish from the lake until you are left with the 
one true fish. (If you came here for good analogies you're probably in the wrong place.)

Let's apply this to our example. 

1. *List all the possible truths.* 
    + $T_1$: Rick is at the conference.
    + $T_2$: Rick is not at the conference. 
2. *List all the possible observations.* 
    + $O_1$: We see someone wearing a lab coat. 
    + $O_2$: We do not see someone wearing a lab coat. 
3. *For each possible truth, define which observations are compatible/incompatible.*
    + If Rick is at the conference, then we will see someone wearing a lab coat: 
    $T_1$ is compatible with $O_1$ and incompatible with everything else (with $O_2$).
    + If Rick is not at the conference, then we might or might not see someone wearing a lab coat: 
    $T_2$ is compatible with $O_1$ and $O_2$, and incompatible with everything else (with nothing).
4. *Make an observation.*  
    1. $O_1$: We see someone wearing a lab coat. 
    2. $O_2$: We do not see someone wearing a lab coat.
5. *Reject all the possible truths that are incompatible with the observation.*
    1. Reject nothing.
    2. Reject $T_1$.
6. *The actual truth is somewhere among the truths that are compatible with the observation.*
    1. $T_1$ or $T_2$: Rick either is at the conference, or he is not.
    2. $T_2$: Rick is not at the conference. 

## The Null and the Alternative

Now, let's apply what we've learned to our `rnorm(n)` working example. Our conclusion 
should be about $\mu$. Our observation is the sample mean $m$ from a random sample of 
size $N$. We just need a model of which $m$ is compatible with 
which $\mu$. Fortunately, we have one: the sampling distribution of $m$. 

*1. List all possible truths.*

We start with all possible truths about $\mu$: it could be *any* number. Recall that we 
can only conclude that $\mu$ is *something* by rejecting that $\mu$ is *everything else*. 
Accordingly, we divide all possible truths about $\mu$ into two parts: the 'something' 
we want to conclude and the 'everything else' that we need to reject in order to do so. 

The 'everything else' we need to reject is the *Null Hypothesis* $H_0$. The 'something' we
conclude in this case is the *alternative hypothesis* $H_1$. There are three fundamental 
versions of $H_0$ and $H_1$:

___

$$
\begin{align*}
& 1a. \qquad  && H_0: \mu \le \mu_0 \qquad && (\mu \text{ is less or equal to some value } \mu_0) \\
&     \qquad  && H_1: \mu  >  \mu_0 \qquad && (\mu \text{ is greater than that value } \mu_0) \\\\
& 1b. \qquad  && H_0: \mu \ge \mu_0 \qquad && (\mu \text{ is greater or equal to some value } \mu_0) \\
&     \qquad  && H_1: \mu  <  \mu_0 \qquad && (\mu \text{ is less than that value } \mu_0) \\\\
& 2.  \qquad  && H_0: \mu  =  \mu_0 \qquad && (\mu \text{ is equal to some value } \mu_0)\\
&     \qquad  && H_1: \mu \ne \mu_0 \qquad && (\mu \text{ is different from that value } \mu_0)
\end{align*}
$$

___

Versions 1a and 1b are called *one-sided tests*. Note that they are basically opposites 
of each other. One-sided tests are rarely used---even though they often should be---due
to a misconception. Version 2 is called a *two-sided test*. Technically, a two-sided test 
is just the combination of both one-sided tests, so it's not really fundamental. You also 
might have heard of 'equivalence tests'. That's also the combination of both one-sided tests, 
but in a different way. But we won't worry about any of that for now. In fact, you can 
erase this whole paragraph from your memory.

Note that $\mu_0$ is just some hypothetical value for $\mu$ that we are particularly 
interested in. For example, you might think that some new medical treatment works better 
than the current standard --- so you hypothesize that the difference of their effects is 
greater than 0. Or, you might think the new medical treatment works *much* better --- 
so you hypothesize that the difference of their effects is greater than 5 (on some scale). 
The difference in treatments corresponds to our $\mu$ and the 0 or 5 to $\mu_0$.

*2. List all the possible observations.*

Our observation will be the sample mean $m$ from a sample of $N=15$. Just like $\mu$, $m$ 
could also be *any* number. 

## The false rejection rate

*3. For each possible truth, define which observations are compatible/incompatible.*

We will do this, using the sampling distribution of $m$.
Recall that the sampling distribution of $m$ is a normal distribution with mean 
$\mu$ and standard deviation $\frac{\sigma}{\sqrt{N}}$.
We will use $N=15$ (the red distribution from **Figure 3**) and we know that $\sigma=1$.
So our sampling distribution is a normal distribution with mean $\mu$ and standard 
deviation $\frac{1}{\sqrt{15}}$.
The general idea is to center this sampling distribution on a hypothetical value $\mu_0$,
so that it has mean $\mu_0$ and standard deviation $\frac{1}{\sqrt{15}}$.
This sampling distribution then tells us what sample mean $m$ to expect if we draw a 
random sample of $N=15$ from a population distribution with $\mu=\mu_0$ and $\sigma=1$.

<!-- NONZERO DISTRIBUTION -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/25_nonzero_distribution.svg" />
  <b> Figure 4. </b> 
  The sampling distribution of \(m\) is never exactly zero. Thus, we cannot the whole
  distribution to define which \(m\) would be compatible with \(\mu=\mu_0\). 
</p>

Simple, right? Not so fast, there is a problem: the sampling distribution is a normal 
distribution, and a normal distribution is never *exactly* zero --- see **Figure 4**.
This has two consequences. First, we *cannot* use the whole sampling distribution of $m$. 
If we did, $\mu=\mu_0$ would be compatible with *any* $m$. In fact, any $\mu$ would be 
compatible with any $m$: no matter where we center the sampling distribution, it always
covers everything from negative to positive infinity. Second, we *should not* use the whole 
sampling distribution of $m$. Take a look at the two zoom boxes in **Figure 4**. 
If our population distribution indeed had mean $\mu=\mu_0$, and we drew a gazillion 
samples from it, how often would $m$ actually fall into these boxes or further 
out? Answer: In less than 1 in 1,200,000,000,000 samples. It seems ridiculous to say that such 
$m$ are compatible with $\mu=\mu_0$. You wouldn't say that getting 39 heads out of 
40 coin flips is compatible with a fair coin, would you? At that point, you'd probably 
think that the coin is manipulated in some way. Similarly, if our $m$ is too extreme, 
we should probably think that $\mu\ne\mu_0$. To recap, it doesn't make sense to use 
the whole sampling distribution to define which $m$ are compatible with a given $\mu$. 
Instead, we will cut off a small part and use *most* of the sampling distribution. 

<!-- NONZERO DISTRIBUTION ALPHA -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/26_nonzero_distribution_alpha.svg" />
  <b> Figure 5. </b> 
  The sampling distribution of \(m\), with a small area cut off. The total gray area is
  \(\alpha\), and the total red area is \(1-\alpha\). If \(\mu=\mu_0\), then \(m\) would fall
  into the gray range in only a fraction \(\alpha\) of cases, in the long run. This is 
  rare enough for us to say that such an \(m\) is incompatible with \(\mu=\mu_0\), so we 
  reject it. If indeed \(\mu=\mu_0\), we will get a compatible \(m\) and thus not reject 
  in \(1-\alpha\) of cases, but we will get an incompatible \(m\) and thus falsely reject
  in \(\alpha\) of cases, in the long run.
</p>

See **Figure 5**. The sampling distribution has a total area of $1$. We cut off a small 
area $\alpha$ (gray) and are left with an area $1-\alpha$ (red). Now we can define which 
$m$ are compatible or incompatible with $\mu=\mu_0$. 

If indeed $\mu=\mu_0$...

+ ...then $m$ would fall into the red range in $1-\alpha$ of all cases 
(i.e. most of the time), in the long run. Those $m$ are compatible with $\mu=\mu_0$.
+ ...then $m$ would fall into the gray range in $\alpha$ of all cases 
(i.e. rarely), in the long run. Those $m$ are incompatible with $\mu=\mu_0$. If our
$m$ falls into this gray range, then we reject $\mu=\mu_0$. 

Of course this means that, if indeed $\mu=\mu_0$, we would commit a *false rejection error* 
in $\alpha$ of all cases, in the long run. So $\alpha$ is our *false rejection rate*. Note 
that we are in full control of $\alpha$, because we decide how much of the sampling 
distribution we cut off. 

There is no easy way to decide what $\alpha$ to use. In the scientific literature, pretty 
much every study uses $\alpha=0.05$, and I will also use it in most of the examples that 
follow. But be aware: there is no good justification to always use 0.05. This is just an 
arbitrary value that has somehow become convention. We will see later that there is a trade-off between 
avoiding false rejections and being able to make true rejections. Intuitively, making it 
harder to reject will protect you against falsely rejecting the actual truth, but it will 
also make it harder to reject everything else. If a smoke detector beeps only when there is 
a lot of smoke, it won't annoy you with false alarms but it also won't be great at 
detecting a fire. So, what is more important to you: avoiding false alarms or getting an 
early warning in the event of a fire? Ideally, you should think about this trade-off when 
setting your $\alpha$. However, we will come back to all of this later. For now, we just  
need to know what $\alpha$ is. 

> ‚úîÔ∏è **Key Point:**  If $\mu$ was indeed equal to some value $\mu_0$, we would falsely 
reject $\mu=\mu_0$ in a fraction $\alpha$ of all cases, in the long-run. Thus, $\alpha$ 
is our *false rejection rate*.

Now, recall that we have divided all possible truths into $H_0$ 
and $H_1$. Both are statements that relate $\mu$ to some value $\mu_0$. We need to reject 
$H_0$ in order to accept $H_1$. So we need to define compatible $m$ for each $H_0$. 

+ $H_0: \mu \le \mu_0 \quad$ (1a)
+ $H_0: \mu \ge \mu_0 \quad$ (1b)
+ $H_0: \mu  =  \mu_0 \quad$ (2)

We already know how to do this for a specific value (case 2), but how should we do this 
for a *range* of values (cases 1a and 1b)? It turns out that the principle is the same. 
Here are the three steps, for all three cases (see **Figure 6**): 

STEP 1. We center the sampling distribution of $m$ on the *most extreme* value under $H_0$, 
i.e. the value at the boundary to $H_1$. This value is $\mu_0$. This distribution now 
tells us what $m$ to expect under $\mu=\mu_0$.

STEP 2. We cut off an area $\alpha=0.05$. We cut off the *most extreme* area, i.e. the area
that reaches furthest into $H_1$.

<!-- H0,H1 -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/06_h0h1.svg" />
  <b> Figure 6. </b> 
  Caption here. 
</p>

STEP 3. The red range are all $m$ that are compatible with $H_0$. The gray range are all the 
$m$ that are incompatible with $H_0$. If $H_0$ was true...

+ Cases 1a, 1b:   
... then $m$ would fall into the red range in *at least* $0.95$ of cases and into the 
gray range in *at most* $0.05$ of cases. If you shift the sampling 
distribution further into $H_0$ but leave the solid line in place, the red area 
increases and the gray area decreases. Thus, our false rejection rate is 
$\alpha \le 0.05$.

+ Case 2:  
... then $m$ would fall into the red range in $0.95$ of cases and into the gray range 
in $0.05$ of cases. Thus, our false rejection rate is $\alpha=0.05$. 

Note that the solid line in **Figure 6** is the least extreme $m$ which would make us
reject $H_0$. 


## The p-value

*4. Make an observation*. 

So now we finally draw our sample of $N=15$ and compute the sample mean $m$.

*5. Reject all the possible truths that are incompatible with the observation.*

This is easy: we just check whether our $m$ is compatible with $H_0$. If not, we reject 
$H_0$ and accept $H_1$. However, we can extract more information than that by computing 
the $p$-value. We do this by repeating the same three steps from the previous section, but
we now use our sample mean $m$ as the cutoff (see **Figure 7**):

1. We center the sampling distribution of $m$ on the *most extreme* value under $H_0$, 
i.e. the value at the boundary to $H_1$. This value is $\mu_0$. This distribution now 
tells us what $m$ to expect under $\mu=\mu_0$.

2. We cut off an area, using the sample mean $m$ as a cutoff. We cut off the 
*most extreme* area, i.e. the area that reaches furthest into $H_1$.

<!-- P VALUE -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/07_pvalue.svg" />
  <b> Figure. </b> Caption here. 
</p>

3. The red area is $1-p$ and the gray area is $p$. Of course, if $p \le \alpha$, our $m$ is 
incompatible with $H_0$ so we reject it. If $H_0$ was true...

    + Case 1a:  
    ... then we would observe an $m$ that is *smaller* than ours in $1-p=0.90$ 
    of all cases and *at least as large* as ours in $p=0.10$ of all cases, in the long run.
    
    + Case 1b:  
    ... then we would observe an $m$ that *larger* than ours in $1-p=0.10$ of all
    cases and *at least as small* as ours in $p=0.90$ of all cases, in the long run.
    
    + Case 2:  
    ... then we would observe an $m$ that is *closer to $\mu_0$* than ours in $1-p=0.80$ 
    of all cases and *at least as far away from $\mu_0$* in $p=0.20$ of all cases. 



<!-- P CURVES -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/14_p_curves.svg" />
  <b> Figure. </b> Caption here. 
</p> 

<!-- CONFIDENCE INTERVALS -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/15_confidence_intervals.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- CONFIDENCE INTERVALS BY N -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/16_confidence_intervals_by_n.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/17_rejection_rate.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- ALPHA BETA INFERENCE -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/18_alpha_beta_inference.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE BY N -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/19_rejection_rate_by_n.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE BY ALPHA -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/20_rejection_rate_by_alpha.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE HEATMAP -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/21_rejection_rate_heatmap.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE HEATMAP BY ALPHA -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/22_rejection_rate_heatmap_by_alpha.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE HEATMAP BY SIGMA -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/23_rejection_rate_heatmap_by_sigma.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- REJECTION RATE HEATMAP BY SIGMA STANDARDIZED -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/24_rejection_rate_heatmap_by_sigma_standardized.svg" />
  <b> Figure. </b> Caption here. 
</p>

