---
# text
title:      "Frequentist Statistics Primer"
subtitle:   "p-values, confidence intervals, and error rates"
excerpt:    "All things frequentist statistics."

# metadata (NB bottom will show "see also: all posts with same tag")
author:     "Aslan B."
date:       2022-05-01
categories: 
- statistics
tags:
- statistics

# other
layout:     single
draft:      false
---

```{r setup, include = FALSE}

# load knitr
library(knitr)
# set chunk opts
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  include = TRUE,
  echo    = TRUE
)
# set seed
set.seed(123)
# display numbers to two decimals
options(digits = 1)

```

```{css, echo=FALSE}
/* make all images centered;
   no width parameter to preserve width set via ggsave;
   fig caption text style is set manually each time,
    there is probably an easier way to do this like
    make a css text class with those style params and
    then just apply that class, but I am too lazy atm
*/
img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

/*  FONT SIZE
    - p = main text
    - li = list (could also use ol and ul for ordered, unordered)
    - h = headings
*/
p {
  font-size: 14px;
  text-align: justify;
  width: auto;
}

li{
  font-size: 14px;
  text-align: justify;
  padding-left: 1em;
}

h1{
  font-size: 11px;
}

h2{
  font-size: 6px;
}

.keypoint{
  border-left: 0.7em solid #cccccc;
  background-color: #e5e5e5;
  padding: 0.75em 1em 0.75em 0.6em;
}

.figure{
  text-align: center;
}

.caption{
  text-align: justify; 
  font-size: 12px;
  margin: -1em 6em 2em 6em;
}

```

```{css, echo=FALSE}
/*
 *  Footnotes
 */
 
footer {
  margin-top: 2ex;
  border-top: 1px solid silver;
  font-size: 1em;
}

footer ol {
  padding-left: 20px;
}

/*
 *  initialiazing a `footnotes` counter on the wrapper
 */
 
article {
  counter-reset: footnotes;
}

/*
 *  inline footnotes references
 *  1. increment the counter at each new reference
 *  2. reset link styles to make it appear like regular text
 */
 
[aria-describedby="footnote-label"] {
  counter-increment: footnotes; /* 1 */
  text-decoration: none; /* 2 */
  color: inherit; /* 2 */
  cursor: default; /* 2 */
  outline: none; /* 2 */
}

/*
 *  actual numbered references
 *  1. display the current state of the counter (e.g. `[1]`)
 *  2. align text as superscript
 *  3. make the number smaller (since it's superscript)
 *  4. slightly offset the number from the text
 *  5. reset link styles on the number to show it's usable
 */
[aria-describedby="footnote-label"]::after {
  content: '[' counter(footnotes) ']'; /* 1 */
  vertical-align: super; /* 2 */
  font-size: 0.8em; /* 3 */
  margin-left: 2px; /* 4 */
  color: #8a0900; /* 5 */
  cursor: pointer; /* 5 */
}

/*
 *  resetting the default focused styles on the number
 */
[aria-describedby="footnote-label"]:focus::after {
  outline: thin dotted;
  outline-offset: 2px;
}

[aria-label="Back to content"] {
  font-size: 0.8em;
}

/* 
 *  highlight target note
 */
footer :target {
  background: yellow;
}

/*
 *  visually hidden yet accessible content
 */
.visually-hidden {
  position: absolute;
  clip: rect(0 0 0 0);
  visibility: hidden;
  opacity: 0;
}

```

In my experience, statistics courses often brush over the fundamentals and quickly---too quickly---move on to teach you different models and how to implement them. So, I decided to write a blogpost explaining the fundamentals of frequentist statistics. The goal is to give you a good understanding of the fundamental frequentist statistical concepts: p-values, confidence intervals, statistical power, etc. Enjoy! 

## In the Long Run

Statistical inference is based on probability theory. Mathematically speaking, probability is a scalar between zero and one, and the probability that any of multiple mutually exclusive events occurs is the sum of their individual probabilities. Intuitively speaking, *high probability* means we expect it and *low probability* means we’d be surprised if it happened. So far so good. But what exactly does probability quantify in the real world?
There are several interpretations of probability. But today, we are dealing with *frequentist statistics*. In this framework, probability refers to hypothetical long run *frequencies*. For example, when flipping a coin, the probability of heads is 0.5. That means, if we repeatedly flipped a coin, the proportion of heads would converge to 0.5 as the number of coin flips approaches infinity. You can see a simulated example in **Figure 1**. 

<!-- plot: law of large numbers -->
<p class="figure">
  <img src="plots/lln.svg" />
</p>
<p class="caption">
  <b> Figure 1. </b> 
  Twenty simulated series of 10,000 coin flips each. The y-axis shows the proportion of heads. 
</p>

More generally, the probability that a process produces a certain outcome is the frequency at which that outcome is observed *in the long run*, i.e. *if the process is repeated infinitely often*. Note that, under this interpretation, it only makes sense to assign probability to repeatable events. Does it make sense to talk about the probability of getting a sum of 12 when rolling three dice? Yes, because you can roll three dice over and over again. Does it make sense to talk about the probability that your hypothesis is true? No, there is nothing to repeat here. 

<p class="keypoint">
<b> ✔️ Probability as a long run frequency </b> <br>
Under the frequentist interpretation, the probability that a process produces an outcome is the frequency at which that outcome is observed <i>in the long run</i>, i.e. <i> if the underlying process is repeated infinitely often </i>.
</p>

## Setup

Statistical inference essentially means inferring some aspect of an unobservable probability distribution---often called the *population distribution*---using a random sample. Step by step, it looks something like this:

1. We are interested in some aspect of the population distribution.  
2. We draw a random sample from that population
3. We use information from the sample to infer something about (some aspect of) the population distribution.

Note that what exactly constitutes the population distribution and the random sample depends on the study design---in particular, which part is random: the inclusion of participants (as in surveys) or the allocation of participants (as in trials).<a href="#whatispop-foot" aria-describedby="footnote-label" id="whatispop-text"></a> But we don't need to worry about that. All that matters for us is the general concept. Maintaining footnotes manually can be a pain. 

Now, we need a specific example to work with, which will be as follows:

1. The population distribution is some distribution. It has an unknown mean $\mu$, which we are interested in. It also has a known standard deviation $\sigma=1$.  
2. We draw a random sample of size $N$ from that population distribution. We denote the sample mean $m$ and the sample standard deviation $s$.  
3. We use the sample mean $m$ to infer something about the population mean $\mu$.  

We will later pretend that *both* $\mu$ and $\sigma$ are unknown, in which case we will have to use both $m$ and $s$. But for now, $\sigma=1$ is known and $s$ can be ignored.

<!-- plot: overview -->
<p class="figure">
  <img src="plots/overview.svg" />
</p>
<p class="caption">
  <b> Figure 2. </b> 
  The working example. We draw a sample of size \(N\) from a population distribution with mean \(\mu\) and standard deviation \(\sigma\). We want to answer some question about \(\mu\). In our case, we know that \(\sigma=1\).
</p>

An easy way to remember what the symbols $\mu,\sigma,m,s,N$ stand for is: Greek letter are for population parameters (Latin ones for sample parameters) and letters that are pronounced with M at the beginning are for the mean (the ones with S are for the standard deviation). 

<table>
  <tr>
    <td>  </td>
    <th> Population </th>
    <th> Sample </th>
  </tr>
  <tr>
    <td> Mean </td>
    <td> \(\mu\) </td>
    <td> \(m\) </td>
  </tr>
  <tr>
    <td> Standard Deviation </td>
    <td> \(\sigma\) </td>
    <td> \(s\) </td>
  </tr>
  <tr>
    <td> Size </td>
    <td>  </td>
    <td> \(N\) </td>
  </tr>
</table>

So, what do we want to know about $\mu$? We might want to know whether it is greater or smaller than some value. We might want to know whether it is different from or similar to some value. Or, we might simply want an estimate of it. To answer any of these questions, we need the sampling distribution. 

## The Sampling Distribution

The sampling distribution exists for any sample statistic (sample mean, median, standard deviation, etc.) and it tells you how that sample statistic would be distributed if you computed it for an infinite series of random samples.

We will be using the sampling distribution of the sample mean $m$. In **Figure 3** you see twenty samples of size $N$, drawn from a population distribution with some mean $\mu$ and some standard deviation $\sigma$. The grey points are the sample values, and the colored squares are the sample means. 

<!-- plot: clt samples -->
<p class="figure">
  <img src="plots/clt_sample.svg" />
</p>
<p class="caption">
  <b> Figure 3. </b> 
  Samples of size \(N\), drawn from a normal distribution with mean \(\mu\) (dashed line) and standard deviation \(\sigma\).
</p>

Now, what if we continued doing this infinitely many times? We would get infinitely many sample means, which would form their own distribution. This is the sampling distribution of $m$. In **Figure 4**, you can see the sampling distribution of $m$ in color (and the underlying population distribution in grey).

<!-- plot: clt distributions -->
<p class="figure">
  <img src="plots/clt_dist.svg" />
</p>
<p class="caption">
  <b> Figure 4. </b> 
  The sampling distribution of \(m\), for three sample sizes \(N\). The samples were drawn from the gray population distribution with mean \(\mu\) and standard deviation \(\sigma\). The dashed line highlights \(\mu\). The x-axis shows the distance from \(\mu\), measured in multiples of \(\sigma\).
</p>

The sampling distribution of $m$ has some important properties. First, it has the same mean as the underlying population distribution—i.e. it has mean $\mu$. Second, its spread decreases as the sample size $N$ increases—specifically, it has standard deviation $\frac{\sigma}{\sqrt{N}}$.<a href="#standarderror-foot" aria-describedby="footnote-label" id="standarderror-text"></a> Third, it is a normal distribution. 

So the sampling distribution of $m$ is a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{N}}$. This is true as long as the population distribution has a defined mean $\mu$ and finite standard deviation $\sigma$, but *the shape of the population distribution does not matter*! This cool fact is called the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). There is a caveat: depending on the shape of the population distribution, the above holds only if $N$ is sufficiently large. For example, a sample size of 5 will probably be enough given a symmetric population distribution but it might not be enough given a heavily skewed population distribution. You can create your own population distribution and simulate the sampling distribution
[here](https://onlinestatbook.com/stat_sim/sampling_dist/). 

### Putting the Sampling Distribution to use

So, now we know what the sampling distribution is, but how is it useful? How can we use that knowledge to learn something about $\mu$? Let’s build some intuition. 

Say we draw a random sample of $N=15$ from our population with unknown mean $\mu$ and standard deviation $\sigma=1$. The sample is shown in **Figure 5**. The sample mean turns out to be $m=0.3$, and it is marked by the solid vertical line.

<!-- plot: sample -->
<p class="figure">
  <img src="plots/sample.svg" />
</p>
<p class="caption">
  <b> Figure 5. </b> 
  Random sample of \(N=15\). The sample mean \(m\) is highlighted by the solid vertical line.
</p>

Let’s ask a simple question: $\mu\stackrel{?}{=}0$. Well, if we assume $\mu\stackrel{!}{=}0$, then our sample mean (solid vertical line) must have come from the distribution highlighted in **Figure 6** (the one with mean $0$). Just by looking at the figure, I think we can agree that this is not unlikely. So, $\mu$ could be $0$. 

<!-- plot: sampling distribution placement 1 -->
<p class="figure">
  <img src="plots/sdistplace1.svg"/>
</p>
<p class="caption">
  <b> Figure 6. </b> 
  Sampling distributions of \(m\) for different \(\mu\), given \(\sigma=1\) and \(N=15\). The highlighted sampling distribution shows how \(m\) would be distributed in the long run if \(\mu=0\). 
</p>

Let’s repeat this with another value: $\mu\stackrel{?}{=}-0.5$. Again, if we assume $\mu\stackrel{!}{=}-0.5$, then our sample mean (solid vertical line) must have come from the distribution highlighted in **Figure 7** (the one with mean $-0.5$). Just by looking at the figure, I think we can agree that this is very unlikely (but not exactly impossible: the tails of the distribution are thin but never zero). So, we can conclude $\mu\ne -0.5$. 

<!-- plot: sampling distribution placement 2 -->
<p class="figure">
  <img src="plots/sdistplace2.svg" />
</p>
<p class="caption">
  <b> Figure 7. </b> 
  Sampling distributions of \(m\) for different \(\mu\), given \(\sigma=1\) and \(N=15\). The highlighted sampling distribution shows how \(m\) would be distributed in the long run if \(\mu=-0.5\). 
</p>

Generally speaking, we could be interested in any value, call it $\mu_0$. If we assume $\mu\stackrel{!}{=}\mu_0$, then our sample mean $m$ must have come from the sampling distribution with mean $\mu_0$. If that is very unlikely, then we conclude $\mu\ne\mu_0$ (if it is not unlikely, we do nothing).

<p class="keypoint">
<b> ✔️ The sampling distribution of the \(m\) </b> <br>
If we (hypothetically) drew infinitely many samples of size \(N\) from a population distribution with mean \(\mu\) and standard deviation \(\sigma\), then all the \(m\)'s would be distributed according to the sampling distribution of \(m\). It is a normal distribution with mean \(\mu\) and standard deviation \(\frac{\sigma}{\sqrt{N}}\). The fact that the sampling distribution of \(m\) and the population distribution both have mean \(\mu\) is important. It allows us to learn something about \(\mu\). For that, we draw a random sample and compute \(m\). We can assume any hypothetical value \(\mu_0\) for the population mean, i.e. \(\mu\stackrel{!}{=}\mu_0\). Under this assumption, our \(m\) must have come from the sampling distribution of \(m\) that has mean \(\mu_0\). If that is very unlikely, we conclude \(\mu\ne\mu_0\).
</p>

## Rejecting the Incompatible

Let's recap. We can use the sampling distribution of $m$ in order to assess if our sample mean $m$ would be likely under the assumption $\mu\stackrel{!}{=}\mu_0$.
If our $m$ would be unlikely under $\mu\stackrel{!}{=}\mu_0$, then we conclude $\mu\ne\mu_0$. If our $m$ *is* likely under $\mu\stackrel{!}{=}\mu_0$, then we do not conclude anything. To understand why, we will look at the logical argument we are making. 

Forget about statistics for a moment. Imagine  we are at a scientific conference, and we want to know whether our common friend Rick is also at the conference. We know that Rick is always wearing a lab coat. We look carefully, but no one is wearing a lab coat. 

1. If Rick is at the conference, then someone will be wearing a lab coat. 
2. No one is wearing a lab coat. 
3. Therefore, Rick is not at the conference.

This is a valid argument called *modus tollens*, also known as 'denying the consequent'. Now, let's say we *do* see someone wearing a lab coat.

1. If Rick is at the conference, then someone will be wearing a lab coat. 
2. Someone is wearing a lab coat. 
3. --

We cannot conclude that Rick was at the conference. We might have seen Rick in his lab coat, or we might have seen someone else wearing a lab coat. We simply cannot conclude anything. 

Below you see these two cases summarized more abstractly. We start with a model/assumption that tells us what observation (O) to expect under a certain possible 
truth (T). Then we make the observation. Finally, if this observation is incompatible with the possible truth, we reject the possible truth.

<table>
  <tr>
    <td>
      <i> 3. Model/Assumption </i>
    </td>
    <td>
      If T, then O.
    </td>
    <td>
      If T, then O.
    </td>
  </tr>
  <tr>
    <td>
      <i> 4. Observation </i>
    </td>
    <td>
      Not O.
    </td>
    <td>
      O.
    </td>
  </tr>
  <tr>
    <td>
      <i> 5. Conclusion </i>
    </td>
    <td>
      Therefore not T.
    </td>
    <td>
      -
    </td>
  </tr>
</table>

This is essentially the logical argument underlying frequentist statistics. However, a few steps are left implicit. Here is the same argument with all steps made explicit:

<table>
  <tr>
    <td style="width:34%">
      <i> 1. List all possible truths. </i> <br>
      \(T_1, T_2, T_3 ... \)
    </td>
    <td colspan="2">
      \(T_1\): Rick is at the conference. <br>
      \(T_2\): Rick is not at the conference.
    </td>
  </tr>
  <tr>
    <td>
      <i> 2. List all possible observations. </i> <br>
      \(O_1, O_2, O_3 ... \)
    </td>
    <td colspan="2">
      \(O_1\): Someone is wearing a lab coat. <br>
      \(O_2\): No one is wearing a lab coat.
    </td>
  </tr>
  <tr>
    <td>
      <i> 3. For each possible truth, define which 
      observations are compatible and which are incompatible. </i> <br>
      <img src="graphics/compatiblemapping.svg">
    </td>
    <td colspan="2">
      If \(T_1\), then \(O_1\). In other words, \(T_1\) is compatible with \(O_1\); it is incompatible 
      with everything else, i.e. with \(O_2\). <br><br>
      If \(T_2\), then \(O_1\) or \(O_2\). In other words, \(T_2\) is compatible with \(O_1\) and \(O_2\);
      it is incompatible with everything else, i.e. with nothing.
    </td>
  </tr>
  <tr>
    <td>
      <i> 4. Make an observation. </i>
    </td>
    <td style="width:33%">
      \(O_1\): Someone is wearing a lab coat.
    </td>
    <td style="width:33%">
      \(O_2\): No one is wearing a lab coat.
    </td>
  </tr>
  <tr>
    <td>
      <i> 5. Reject all possible truths that are incompatible with the observation. </i>
    </td>
    <td style="width:33%">
      Reject nothing.
    </td>
    <td style="width:33%">
      Reject \(T_1\). 
    </td>
  </tr>
  <tr>
    <td>
      <i> 6. The actual truth is somewhere among the remaining possible truths (which
      are compatible with the observation). </i>
    </td>
    <td style="width:33%">
      \(T_1\) or \(T_2\): Rick is or is not at the conference.
    </td>
    <td style="width:33%">
      \(T_2\): Rick is not at the conference.
    </td>
  </tr>
</table>

So we start with all possible truths, then make an observation, and then *reject* the possible truths that are *incompatible* with our observation. We are left with all possible truths that have not been rejected, and the actual truth is somewhere in there. The key point is: the only way to conclude that the actual truth is *something* is to reject *everything else*. You can only reject! (Hence why, in the previous section, we could conclude $\mu\ne\mu_0$ but we could never conclude $\mu=\mu_0$). 

Now, if we want to reject the possible truths that are incompatible with our observation, then what does 'incompatible' mean? In the conference example, it meant 'logically impossible'. If Rick (who is always wearing a lab coat) is at the conference, then it is logically impossible that no one there will be wearing a lab coat. But most questions cannot be solved by logic alone---we need probability and statistics. Here, incompatible means 'improbable' or 'unlikely'. And in frequentist statistics, this translates to a small long run frequency. So, we want to reject all possible truths under which our observation would be rare, in the long run. 

## The Null and the Alternative

Back to our working example: we are interested in the population mean $\mu$, and we observe a random sample of size $N$ with sample mean $m$. We will now walk through our argument step by step. We will $N=15$ (same as earlier).

*1. List all possible truths.*

We start with all possible truths about $\mu$: it could be any number whatsoever. But we're not done yet. Recall that we can only conclude that $\mu$ is *something* by rejecting *everything else*. Accordingly, we split all possible truths about $\mu$ into the 'something' we want to conclude and the 'everything else' that we need to reject in order to do so. The 'everything else' we need to reject is the *Null Hypothesis* $H_0$. The 'something' we then conclude is the *alternative hypothesis* $H_1$. There are three fundamental versions of $H_0$ and $H_1$:

<table>
  <tr>
    <td>
    1a
    </td>
    <td>
    \(H_0: \mu \le \mu_0\) <br>
    \(H_1: \mu  >  \mu_0\)
    </td>
    <td>
    \(\mu\) is less or equal to some value \(\mu_0\) <br>
    \(\mu\) is greater than some value \(\mu_0\)
    </td>
  </tr>
  <tr>
    <td>
    1b
    </td>
    <td>
    \(H_0: \mu \ge \mu_0\) <br>
    \(H_1: \mu  <  \mu_0\)
    </td>
    <td>
    \(\mu\) is greater or equal to some value \(\mu_0\) <br>
    \(\mu\) is less than some value \(\mu_0\)
    </td>
  </tr>
  <tr>
    <td>
    2
    </td>
    <td>
    \(H_0: \mu  =  \mu_0\) <br>
    \(H_1: \mu \ne \mu_0\)
    </td>
    <td>
    \(\mu\) is equal to some value \(\mu_0\) <br>
    \(\mu\) is different from some value \(\mu_0\)
    </td>
  </tr>
</table>

Versions 1a and 1b are called *one-sided tests*. Note that they are basically opposites of each other. Version 2 is called a *two-sided test*. Technically, a two-sided test is just the combination of both one-sided tests, so it's not really fundamental. 

Recall that $\mu_0$ is just some hypothetical value for $\mu$ that we are particularly interested in. We are often interested in specific hypothetical values for a given parameter. Say you want to know if a vaccine prevents more than 90% of infections. Your parameter is vaccine efficacy and you are asking if it's greater than 90%. Or, say you want to know if coffee changes your reaction time. Your parameter is the change in reaction time due to coffee consumption and you are asking if it's different from zero. Of course, sometimes you are not interested in specific hypothetical values for a given parameter. Say you conduct a survey and ask people about their age. Your parameter is the population age, but you are simply asking what it is---there is no value of interest. As we will see shortly, that situation also corresponds to one of the versions above. 

Now that we have listed all possible truths about $\mu$ in the form of $H_0$ and $H_1$, we can move on to the next step. 

*2. List all possible observations.*

We continue with all possible sample means $m$: again, it could by any number whatsoever. Done. 

## The false rejection rate

*3. For each possible truth, define which observations are compatible and which are incompatible.*

Since we want to reject $H_0$, we need to define which $m$ would be incompatible with $H_0$. In the frequentist framework, incompatible means 'rare, in the long run'. But how rare, exactly? We control that, by setting the parameter $\alpha$. We will take a closer look at it in a second, but for now I will just go ahead and set $\alpha=0.05$. With that, we have defined which $m$ are incompatible with $H_0$: those that we would get in no more than 5% of cases in the long run, if $H_0$ were true. This incompatible region is shown in **Figure 7**; see the caption for more details. 

<!-- plot: h0h1 -->
<p class="figure">
  <img src="plots/h0h1.svg" />
</p>
<p class="caption">
  <b> Figure 7. </b> 
  Set-up for one-sided tests (left and center) and two-sided tests (right), with \(\alpha=0.05\). The top row shows \(H_0\) and \(H_1\). The bottom row splits all possible \(m\) into those compatible with (cw) and those incompatible with (icw) \(H_0\). The incompatible region starts at the solid vertical line, whose position is determined by the gray area (\(\alpha\)). In the one-sided case, \(m\) would fall into the incompatible region in <i>at most</i> 5% of cases in the long run, if \(H_0\) were true (5% for the highlighted distribution, which is the most extreme version of \(H_0\); less than 5% for any distribution shifted further into \(H_0\)). For the two-sided case, \(m\) would fall into the incompatible region in <i>exactly</i> 5% of cases in the long run, if \(H_0\) were true.
</p>

So, by setting the parameter $\alpha$, we define what it takes to reject $H_0$. In **Figure 7**, you can see that $\alpha$ is the gray area of the highlighted sampling distribution---decreasing it will shift the incompatible region further away from $H_0$. So, $\alpha$ defines how far away from $H_0$ the sample mean $m$ has to fall, in order to reject $H_0$.

Of course, $\alpha$ is a long run frequency. If $H_0$ were true, then $m$ would fall into the incompatible region in at most a fraction $\alpha$ of cases, in the long run. Put differently, if $H_0$ were true, then we would (falsely) reject $H_0$ in at most a fraction $\alpha$ of cases, in the long run. Thus, $\alpha$ is our *false rejection rate*.

What should we set $\alpha$ to? So far I have simply set it to 0.05, and we will stick with 0.05 as we move on. In fact, most studies simply set it to 0.05. But what is the justification? There is none! There is no good justification for always using 0.05. In fact, there is no good justification for always using *any* specific value. Instead, you should set $\alpha$ depending on the context of the research question and the available resources. We will look at this in more detail, later. For now, it's sufficient to know the concept.

<p class="keypoint">
<b>✔️ The false rejection rate \(\alpha\)</b><br>
If \(H_0\) were true, then we would falsely reject it in a small fraction \(\alpha\) of all cases, in the long run. Thus, \(\alpha\) is our <i>false rejection rate</i>. We are in full control of \(\alpha\), i.e. we decide what the false rejection rate is. 
</p>

## The p-value

*4. Make an observation.*

Next, we draw our sample of $N=15$ and compute the sample mean $m$. 

*5. Reject all possible truths that are incompatible with the observation.*

Recall that, by setting $\alpha$, we have defined how far away from $H_0$ the incompatible region starts. Now, we check how far away from $H_0$ our sample mean $m$ actually is, by computing the $p$-value. This is shown in **Figure 8**; see the caption for more details. If $p\le\alpha$, our sample mean $m$ is in the incompatible region, and we reject $H_0$. 

<!-- plot: p-values -->
<p class="figure">
  <img src="plots/pvalue.svg" />
</p>
<p class="caption">
  <b> Figure 8. </b>
  \(p\)-values for one-sided tests (left and center) and the two-sided test (right). The top row shows \(H_0\) and \(H_1\). The middle row shows the sample with the sample mean \(m\). The bottom row shows the corresponding \(p\)-values. The gray area (\(p\)) is determined by the position of the solid vertical line (\(m\)). In the one-sided case (left / center), \(m\) would fall farther away from \(H_0\) (than the \(m\) from our sample) in <i>at most</i> 10% / 90% of all cases in the long run, if \(H_0\) were true (10% / 90% for the highlighted distribution, which is the most extreme version of \(H_0\); less than 10% / 90% for any distribution shifted further into \(H_0\)). For the two-sided case, \(m\) would fall farther away from \(H_0\) (than the \(m\) from our sample) in <i>exactly</i> 20% of all cases in the long run, if \(H_0\) were true. 
</p>

So by comparing $p$ to $\alpha$, we check if our rejection criterion is met. You can see that the visualization of $\alpha$ (**Figure 7**) is very similar to the visualization of $p$ (**Figure 8**)---in fact, if $p=\alpha$, then the visualizations are identical. Of course, we *set* $\alpha$ but we *compute* $p$. By setting $\alpha$, we answer the question: how far away from $H_0$ would $m$ need to fall, in order to reject $H_0$? By computing $p$, we answer the question: how far away from $H_0$ did $m$ actually fall?

As you may have noticed, $p$ is also a long run frequency. If $H_0$ were true, then $m$ would fall farther from $H_0$ (than the $m$ from our sample) in at most a fraction $p$ of cases, in the long run.<a href="#onesidedp-foot" aria-describedby="footnote-label" id="onesidedp-text"></a>

<p class="keypoint">
<b>✔️ The \(p\)-value</b><br>
If \(H_0\) were true, then \(m\) would fall farther away from \(H_0\) (than the \(m\) from our sample) in a fraction \(p\) of all cases, in the long run. If \(p\le\alpha\), then we reject \(H_0\).
</p>

## The Confidence Curve

So far, we have computed the $p$-value for $H_0$. To that end, we have placed the sampling distribution on $\mu_0$ (the most extreme value under a one-sided Null hypotheses $H_0:\mu\le\mu_0$ or $H_0:\mu\ge\mu_0$, and the only value under a two-sided Null Hypothesis $H_0:\mu=\mu_0$).  

Well, we can repeat that exercise but use any other value instead of $\mu_0$, and compute a $p$-value. In fact, we can repeat that exercise for *all* possible values for $\mu$. If we plot the $p$-values for all possible values, we get what is often called a *confidence curve*. This is shown in **Figure 9**. 

<!-- plot: p-value curves -->
<p class="figure">
  <img src="plots/pvaluecurve.svg" />
</p>
<p class="caption">
  <b> Figure 9. </b> 
  Confidence curves for one-sided tests (left and center) and the two-sided test (right). The top row shows the sample with sample mean \(m\) (same as before). The middle rows show \(p\)-values for three arbitrary values. The bottom row shows the confidence curves, which consist of \(p\)-values for all possible values.
</p> 

## The Confidence Interval

Now that we have the $p$-values for each possible value that $\mu$ could take, we can just reject all values with $p\le\alpha$. All the non-rejected values for $\mu$ form what is usually called the $(1-\alpha)\times 100\%$ *confidence interval*. Since we set $\alpha=0.05$, we have a 95% confidence interval. This is shown in **Figure 10**.

<!-- plot: confidence intervals -->
<p class="figure">
  <img src="plots/ci.svg" />
</p>
<p class="captions">
  <b> Figure 10. </b>
  Caption here. 
</p>

<!-- plot: confidence intervals by n -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/ci_n.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrate.svg" />
  <b> Figure. </b> Caption here. 
</p>

<table>
  <tr>
    <th>
      Decision
    </th>
    <th>
      Truth
    </th>
    <th>
      Evaluation
    </th>
    <th>
      Rate
    </th>
  </tr>
    <td rowspan="2">
      Rejection
    </td>
    <td>
      Value in question is the true value.
    </td>
    <td>
      False
    </td>
    <td>
      \(\alpha\)
    </td>
  <tr>
    <td>
      Value in question is not the true value.
    </td>
    <td>
      True
    </td>
    <td>
      \(1-\beta\)
    </td>
  </tr>
  <tr>
    <td rowspan="2">
      Non-rejection
    </td>
    <td>
      Value in question is the true value.
    </td>
    <td>
      True
    </td>
    <td>
      \(1-\alpha\)
    </td>
  </tr>
  <tr>
    <td>
      Value in question is not the true value.
    </td>
    <td>
      False
    </td>
    <td>
      \(\beta\)
    </td>
  </tr>
</table>

<!-- plot: alpha beta inference -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrate_inference.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate by alpha -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrate_alpha.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate by n -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrate_n.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate heatmap by alpha -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrateheat_alpha.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate heatmap by sigma -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrateheat_sigma.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- plot: rejection rate heatmap by sigma standardized -->
<p style="text-align: center; font-size: 12px">
  <img src="plots/rejrateheat_sigma_std.svg" />
  <b> Figure. </b> Caption here. 
</p>

<!-- footnotes -->
<footer>
<h2 class="visually-hidden" id="footnote-label"> Footnotes </h2>
<ol>

<li id="whatispop-foot">
In surveys, participants may be included at random, in which case the population refers to all people that could have been drawn. In trials, participants are typically not included at random but are randomly allocated to group, in which case the population refers to all possible allocations of the included participants. It is pretty rare that participants are both included and allocated at random, but in this case the population would refer to all possible allocations of all people that could have been included. 
<a href="#whatispop-text" aria-label="Back to content"> ↩ </a>
</li>

<li id="standarderror-foot">
The standard deviation of the sampling distribution is also called the standard error. 
<a href="#standarderror-text" aria-label="Back to content"> ↩ </a>
</li>

<li id="onesidedp-foot">
"If \(H_0\) were true, then \(m\) would fall farther from \(H_0\) (than the \(m\) from our sample) in at most a fraction \(p\) of cases, in the long run." If we want to be super-duper accurate, then this definition of the \(p\)-value is a little ambiguous for the one-sided case (but perfectly fine for the two-sided case). For the one sided cases, it would be more accurate to say: if \(H_0\) were true, then \(m\) would be larger / smaller in only a fraction \(p\) of cases in the long run. The difference becomes apparent when \(m\) falls "into \(H_0\)", see the middle column in the figure below. <img src="plots/pvalue.svg" /> Part of the gray area is still "inside \(H_0\)", so it doesn't really make sense to say that the values under the gray area are farther away from \(H_0\) than \(m\) (solid line). Yet, the values under the gray area are definitely smaller than \(m\). However, the definition given in the main text is more universal and it communicates the concept better. So, I think we can keep this little caveat in mind and stick to that definition: if \(H_0\) were true, then \(m\) would fall farther from \(H_0\) (than the \(m\) from our sample) in at most a fraction \(p\) of cases, in the long run.
<a href="#onesidedp-text" aria-label="Back to content"> ↩ </a>
</li>

</ol>
</footer>
